\begin{thebibliography}{9}

% Author, A. A., & Author, B. B. (Date of publication). Title of article. Title of Online Periodical, volume number(issue number if available). Retrieved from 
% http://www.someaddress.com/full/url/

%Authors, author initials [See the Reference Formats tab for examples where there are more than one author.]
%Article title
%Journal Title
%Volume
%Issue Number
%Year of Publication
%Page Numbers
%Website Dibble, B., and B. Milech, 'Elizabeth Jolley Research Collection', http://john.curtin.edu.au/jolley/index.html, 2008, (accessed 1 August 2010).  

%% Introduction
\bibitem{DDSC}
  J.Z Kolter, S Batra, A. Ng. \emph{Energy disaggregation via discriminative sparse coding}.
  Adv. Neural. Inform. Process. p.1–9, 2010.

\bibitem{commision}
Comission of the European Communities, \emph{Action Plan for Energy Effciency: Realising the Potential}, COM 545 final, 2006.

\bibitem{jon}
Jon Froehlich, Eric Larson, Sidhant Gupta, Gabe Cohn, Matthew S. Reynolds, Shwetak N. Patel. \emph{Disaggregated End-Use Energy Sensing for the Smart Grid}. 
IEEE Pervasive Computing, Special Issue on Smart Energy Systems, 2011.

\bibitem{hart}
G. Hart. \emph{Nonintrusive appliance load monitoring}. 
Proceedings of the IEEE, 80(12), 1992.

\bibitem{load}
W. Lee, G. Fung, H. Lam, F. Chan, and M. Lucente. \emph{Exploration on load signatures}. 
International Conference on Electrical Engineering (ICEE), 2004.

\bibitem{patel}
S. N. Patel, T. Robertson, J. A. Kientz, M. S. Reynolds, and G. D. Abowd. \emph{At the flick of a switch: Detecting and classifying unique electrical events on the residential power line}. 
9th international conference on Ubiquitous Computing (UbiComp), 2007.

\bibitem{blondel}
Mathieu Blondel, Kazuhiro Seki, Kuniaki Uehara. \emph{Block Coordinate Descent Algorithms for Large-scale Sparse Multiclass Classification}, 2013

\bibitem{block2}
Yangyang Xu and Wotao Yin. \emph{A Block Coordinate Descent Method for Regularized Multiconvex Optimization with Applications to Nonnegative Tensor Factorization and Completion}. 
Vol. 6, No. 3.  Society for Industrial and Applied Mathematics, 2013, pp. 1758–1789.

\bibitem{reduce}
B. Neenan and J. Robinson. \emph{Residential electricity use feedback: A research synthesis and economic framework}. 
Technical report, Electric Power Research Institute, 2009.

% Optimization
\bibitem{convex}
Boyd, Stephen P, Vandenberghe, Lieven. \emph{Convex Optimization}. 
Cambridge University Press. p. 129, 2004

% - Machine Learning
\bibitem{ainorvig}
Russell, Stuart; Norvig, Peter. \emph{Artificial Intelligence: A Modern Approach (2nd ed.)}. 
Prentice Hall, 2003.

\bibitem{bishop}
C. M. Bishop. \emph{Pattern Recognition and Machine Learning}. 
Springer, 2006.

\bibitem{bengio}
Yoshua Bengio. \emph{Learning Deep Architectures for AI}. 
Now Publishers Inc. pp. 1–3, 2009.

% - Classification
\bibitem{singla}
P. Singla and P. Domingos. \emph{Discriminative training of Markov logic networks}. 
In AAAI, 2005.

\bibitem{mccallum}
J. Lafferty, A. McCallum, and F. Pereira. \emph{Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data}. 
In ICML, 2001.

\bibitem{mjordan}
A. Ng and M. I. Jordan. \emph{On Discriminative vs. Generative Classifiers: A Comparison of Logistic Regression and Naive Bayes}. 
In NIPS, 2001.

% - Deep Learning
\bibitem{deep}
Deng and D. Yu, \emph{Deep Learning: Methods and Applications} http://research.microsoft.com/pubs/209355/DeepLearning-NowPublishing-Vol7-SIG-039.pdf, 2014.

\bibitem{deep2}
Bengio, Yoshua. \emph{Learning Deep Architectures for AI}. Foundations and Trends in Machine Learning 2 (1), 2009.

\bibitem{deep3}
Song, Hyun Ah, and Soo-Young Lee. \emph{Hierarchical Representation Using NMF}. Neural Information Processing. Springer Berlin Heidelberg, 2013.

\bibitem{nervous}
Olshausen, Bruno A. \emph{Emergence of simple-cell receptive field properties by learning a sparse code for natural images.} Nature 381.6583: 607-609, 1996.

% - Dimensionality Reduction
\bibitem{dr}
Roweis, S. T., Saul, L. K. \emph{Nonlinear Dimensionality Reduction by Locally Linear Embedding}. Science 290 (5500): 2323–2326, 2000.

\bibitem{samet}
Samet, H.  \emph{Foundations of Multidimensional and Metric Data Structures}. Morgan Kaufmann, 2006.

\bibitem{fodor}
Fodor,I. \emph{A survey of dimension reduction techniques}. Center for Applied Scientific Computing, Lawrence Livermore National, Technical Report, 2002


%% ANN

\bibitem{1943}
McCulloch, Warren; Walter Pitts. \emph{A Logical Calculus of Ideas Immanent in Nervous Activity}. Bulletin of Mathematical Biophysics 5 (4): 115–133, 1943.

\bibitem{perceptron}
Rosenblatt, F. \emph{The Perceptron: A Probabilistic Model For Information Storage And Organization In The Brain}. Psychological Review 65 (6): 386–40, 1958.

\bibitem{brain}
Russell, Ingrid. \emph{Neural Networks Module}, 2012.

\bibitem{nonlinear}
K. Hornik, M. Stinchcombe, and H. White, `\emph{Multilayer feedforward networks are universal approximators}, Neural Networks, vol. 2, no. 5, pp. 359-366, 1989.

\bibitem{svm}
R. Collobert and S. Bengio. \emph{Links between Perceptrons, MLPs and SVMs}. Proc. Int'l Conf. on Machine Learning (ICML), 2004.

\bibitem{backpro}
Rumelhart, David E., Geoffrey E. Hinton, and R. J. Williams. \emph{Learning Internal Representations by Error Propagation}. Parallel distributed processing: Explorations in the microstructure of cognition, Volume 1: Foundations. MIT Press, 1986.

\bibitem{hidden}
Sepp Hochreiter and Jürgen Schmidhuber. \emph{Feature extraction through LOCOCODE}. 
Neural Computation, 11(3):679-714, 1999.

% - Autoencoder

\bibitem{autoencoder_image}
Stanford, http://ufldl.stanford.edu/wiki/index.php/Autoencoders\textunderscore and\textunderscore Sparsity, 7 April 2013, (accesed 10 July 2015)

\bibitem{autoencoder2}
Hinton, G. E. and Salakhutdinov, R. R. \emph{Reducing the dimensionality of data with neural networks}. Science 2006.

% - sparse coding
\bibitem{scprimate}
P F\"oldiák,  \emph{Sparse coding in the primate cortex , in The Handbook of Brain Theory and Neural Networks}, 
Second Edition, pp 1064-1068, ed. Michael A. Arbib, MIT Press, 2002.

\bibitem{sc}
B A Olshausen, D J Field, \emph{Sparse coding with an overcomplete basis set}: Vision Research, 37:3311-3325, 1997.

\bibitem{mclaren}
I P L McLaren, N J MacKintosh, \emph{Associative learning and elemental representation: II. Generalization and discrimination}, Animal Learning \& Behavior, 30(3):177-200, 2002.

\bibitem{olshausen}
Olshausen and Field. \emph{Emergence of simple-cell receptive field properties by learning a sparse code for natural images}, 1996.

\bibitem{willshaw}
D Willshaw, P Dayan P. \emph{Optimal plasticity from matrix memories: what goes up must come down}, 
Neural Computation 2:85-93, 1990.

\bibitem{cover}
T M Cover, J A Thomas, \emph{Elements of Information Theory}, 22nd edition, Wiley-Interscience, 2006.

\bibitem{hoyer}
P.O. Hoyer. \emph{Non-negative sparse coding}. 
In IEEE Workshop on Neural Networks for Signal Processing, 2002.

\bibitem{zou}
Hui Zou and Trevor Hastie and Robert Tibshirani, \emph{Sparse Principal Component Analysis}, 
Journal of Computational and Graphical Statistics, 2006.

%- Ridge regresssion
\bibitem{ill-posed}
Tikhonov A.N., Leonov A.S., Yagola A.G., \emph{Nonlinear Ill-Posed Problems}, V. 1, V. 2, Chapman and Hall, 1998.

% - Lasso
\bibitem{lasso}
Tibshirani, R. \emph{Regression shrinkage and selection via the LASSO}. J. Royal. Statist. Soc B., Vol. 58, No. 1, pages 267-288, 1996.

\bibitem{bayes_lasso}
T. Park, G. Casella. \emph{The Bayesian Lasso}. American Statistical Association, Journal of the American Statistical Association
June 2008, Vol. 103, No. 482, 2008.

\bibitem{gradient}
Jan A. Snyman. \emph{Practical Mathematical Optimization: An Introduction to Basic Optimization Theory and Classical and New Gradient-Based Algorithms}. Springer Publishing, 2005.
%% problem def


\bibitem{awesome}
A collection of open datasets. https://github.com/caesar0301/awesome-public-datasets, (accessed 10 Feb 2015)

%% METHOD
\bibitem{eia}
Energy Information Administration (EIA), frequently asked questions, 20 February 2015,
http://www.eia.gov/tools/faqs/faq.cfm?id=97\&t=3, (accessed 10 Mars 2015).

% datasets
\bibitem{pecan}
Pecan Street Inc. 'Dataport'. https://dataport.pecanstreet.org/, (accessed 12 Mars 2015).

\bibitem{collins}
M. Collins. \emph{Discriminative training methods for hidden markov models: Theory and experiements with perceptron algorithms}. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, 2002.

%% Conclusion

%- algorithm
\bibitem{grouplasso}
M. Yuan and Y. Lin. \emph{Model selection and estimation in regression with grouped variables}. 
Journal of the Royal Statisical Society, Series B, 68(1):49–67, 2007.

\bibitem{kotler2}
Kolter, J. Z., and Jaakkola, T. \emph{Approximate Inference in Additive Factorial HMMs}. In International Conference on Artificial Intelligence and Statistics, 2012.

\bibitem{hyper}
James S. Bergstra, Remi Bardenet, Yoshua Bengio, and Balazs Kegl. \emph{Algorithms for hyper-parameter optimization}. In Advances in Neural Information Processing Systems 25, 2011.

\bibitem{google}
Le, Q., Ranzato, M., Monga, R., Devin, M., Chen, K., Corrado, G., et al. \emph{Building high-level features using large scale unsupervised learning}. In ICML, 2012.

\bibitem{dropout}
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
\emph{Dropout: A simple way to prevent neural networks from overfitting}. JMLR, 2014.

\bibitem{scikit}
\emph{Scikit-learn: Machine Learning in Python}, Pedregosa et al., JMLR 12, pp. 2825-2830, 2011.


\end{thebibliography}

